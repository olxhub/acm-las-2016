{
  "start": [
    4089, 
    7620, 
    10730, 
    13789, 
    18550, 
    21500, 
    25770, 
    31109, 
    35899, 
    40480, 
    44969, 
    50829, 
    55520, 
    60289, 
    64328, 
    66899, 
    72560, 
    76479, 
    80170, 
    83700, 
    89079, 
    92799, 
    96249, 
    99469
  ], 
  "end": [
    7620, 
    10730, 
    13789, 
    18550, 
    21500, 
    25770, 
    31109, 
    35899, 
    40480, 
    44969, 
    50829, 
    55520, 
    60289, 
    64329, 
    66899, 
    72560, 
    76479, 
    80170, 
    83700, 
    89079, 
    92799, 
    96249, 
    99469, 
    104500
  ], 
  "text": [
    "To understand our results, we first need to go over the context of the case study the", 
    "data is from. We used data from an introductory computer", 
    "science course Focusing on the code reading comprehension", 
    "assessments in 9 lab assignments We had two raters categorize and label the wrong answers", 
    "The raters labeled the Top-100 wrong answers", 
    "for each lab, which ranged from 3 to 19% of all the distinct wrong", 
    "answers per lab while covering 50 to 100% of the student\u2019s responses.", 
    "For evaluation we also took a random 50 student sample from each lab and labeled all of those", 
    "distinct wrong answers. This let us measure precision and recall. Using the", 
    "first part of our model, which only directly applied our student misunderstanding definition,", 
    "we achieved perfect precision and 0.6 to 1.0 recall across the labs", 
    "when applying both parts of the model, we evaluated 4 co-occurrence metrics against", 
    "an uninformed baseline, which is what I will spend the rest of this video on.", 
    "To create our uninformed baseline, we simply took the original propagation conditions", 
    "and replaced the third condition with probability p. This", 
    "allowed us to simply flip a biased coin rather than take into account co-occurrence.", 
    "To compare the different metrics and the baseline, we used the area under the curve. However", 
    "we had to modify it because of our perfect precision and high recall when we applied", 
    "only the first definition part of the model, which usually gave us a curve", 
    "like this. To fix this we anchored  our graph at the corners where p=0 and p = 1", 
    "and then scaled the resulting graph into a 1x1 square", 
    "and took the area under this curve. In our results", 
    "Across 9 labs and 4 co-occurrence metrics, the baseline", 
    "was out performed 35 out of 36 times and by an average factor of 1.8"
  ]
}