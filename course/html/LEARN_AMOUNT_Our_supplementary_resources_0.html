<p>Please consider visiting our other presentations at L@S and LAK this week. The ASSISTments Team schedule is below. We look forward to chatting with you and answering any questions you may have!<br /><br /><br /><strong>MONDAY 4/25 - 5:30PM POSTER SESSION</strong> <br /><br /><span style="text-decoration: underline;">Studying Learning at Scale with the ASSISTments TestBed</span> <br />Korinn S. Ostrow, Neil T. Heffernan<br /><br />Keywords: ASSISTments TestBed; Randomized Controlled Experimentation at Scale; Authentic Learning Environments; Assessment of Learning Infrastructure<br /><br />Abstract: An interactive demonstration on how to design and implement randomized controlled experiments at scale within the ASSISTments TestBed, a new collaborative for educational research funded by the National Science Foundation (NSF). The Assessment of Learning infrastructure (ALI), a unique data retrieval and analysis tool, is also demonstrated.<br /><br /></p>
<p><span style="text-decoration: underline;">ASSISTments Dataset from Multiple Randomized Controlled Experiments</span><br />Douglas Selent, Thanaporn Patikorn, Neil Heffernan<br /><br />Keywords: ASSISTments; Randomized Controlled Experiments; Dataset<br /><br />Abstract: In this paper, we present a dataset consisting of data generated from 22 previously and currently running randomized controlled experiments inside the ASSIStments online learning platform. This dataset provides data mining opportunities for researchers to analyze ASSISTments data in a convenient format across multiple experiments at the same time. The data preprocessing steps are explained in detail to inform researchers about how this dataset was generated. A list of column descriptions is provided to define the columns in the dataset and a set of summary statistics are presented to briefly describe the dataset.</p>
<p><br /><span style="text-decoration: underline;">The Opportunity Count Model: A Flexible Approach to Modeling Student Performance</span><br />Yan Wang, Korinn Ostrow, Seth Adjei, Neil Heffernan<br /><br />Keywords: Opportunity Count; Random Forest; Student Modeling; Next Problem Correctness; Intelligent Tutoring System<br /><br />Abstract: Detailed performance data can be exploited to achieve stronger student models when predicting next problem correctness (NPC) within intelligent tutoring systems. However, the availability and importance of these details may differ significantly when considering opportunity count (OC), or the compounded sequence of problems a student experiences within a skill. Inspired by this intuition, the present study introduces the Opportunity Count Model (OCM), a unique approach to student modeling in which separate models are built for differing OCs rather than creating a blanket model that encompasses all OCs. We use Random Forest (RF), which can be used to indicate feature importance, to construct the OCM by considering detailed performance data within tutor log files. Results suggest that OC is significant when modeling student performance and that detailed performance data varies across OCs.<br /><br /><strong>TUESDAY 4/26 - 1:30PM SESSION #5 (PENTLAND E+W)</strong><br /><br />5A: AXIS: G<span style="text-decoration: underline;">enerating Explanations at Scale with Learnersourcing and Machine Learning</span> <br />Joseph Jay Williams, Juho Kim, Anna Rafferty, Samuel Maldonado, Krzysztof Z. Gajos, Walter S. Lasecki, Neil Heffernan<br /><br />Keywords: Explanation; learning at scale; crowdsourcing; learnersourcing; machine learning; adaptive learning<br /><br />Abstract: While explanations may help people learn by providing information about why an answer is correct, many problems on online platforms lack high-quality explanations. This paper presents AXIS (Adaptive eXplanation Improvement System), a system for obtaining explanations. AXIS asks learners to generate, revise, and evaluate explanations as they solve a problem, and then uses machine learning to dynamically determine which explanation to present to a future learner, based on previous learners' collective input. Results from a case study deployment and a randomized experiment demonstrate that AXIS elicits and identifies explanations that learners find helpful. Providing explanations from AXIS also objectively enhanced learning, when compared to the default practice where learners solved problems and received answers without explanations. The rated quality and learning benefit of AXIS explanations did not differ from explanations generated by an experienced instructor.<br /><br /><strong>THURSDAY 4/28 - 10:30AM LAK SESSION "THEORETICAL AND CONCEPTUAL MODELS" (PRESONTFIELD</strong>)<br /><br />4D2: <span style="text-decoration: underline;">The Assessment of Learning Infrastructure (ALI): The Theory, Practice, and Scalability of Automated Assessment</span><br />Korinn Ostrow, Doug Selent, Yan Wang, Eric Van Inwegen, Neil Heffernan, Joseph Jay Williams<br /><br />Keywords: Assessment of Learning Infrastructure, Automated Analysis, Randomized Controlled Experiments at Scale, The ASSISTments TestBed, Universal Data Reporting, Tools for Learning Analytics.<br /><br />Abstract: Researchers invested in K-12 education struggle not just to enhance pedagogy, curriculum, and student engagement, but also to harness the power of technology in ways that will optimize learning. Online learning platforms offer a powerful environment for educational research at scale. The present work details the creation of an automated system designed to provide researchers with insights regarding data logged from randomized controlled experiments conducted within the ASSISTments TestBed. The Assessment of Learning Infrastructure (ALI) builds upon existing technologies to foster a symbiotic relationship beneficial to students, researchers, the platform and its content, and the learning analytics community. ALI is a sophisticated automated reporting system that provides an overview of sample distributions and basic analyses for researchers to consider when assessing their data. ALI&rsquo;s benefits can also be felt at scale through analyses that crosscut multiple studies to drive iterative platform improvements while promoting personalized learning.<br /><br /><strong>FRIDAY 4/29 - 1:00PM LAK SESSION "PREDICTIVE MODELLING" (PENTLAND WEST)</strong><br /><br />7D2: <span style="text-decoration: underline;">Predicting Student Performance on Post-requisite Skills Using Prerequisite Skill Data: An alternative method for refining Prerequisite Skill Structures</span><br />Seth Adjei, Anthony Botelho, Neil Heffernan<br /><br />Keywords: Prerequisite Structures, learning maps, skill relationships, refinements, PLACEments<br /><br />Abstract: Prerequisite skill structures have been closely studied in past years leading to many data-intensive methods aimed at refining such structures. While many of these proposed methods have yielded success, defining and refining hierarchies of skill relationships are often difficult tasks. The relationship between skills in a graph could either be causal, therefore, a prerequisite relationship (skill A must be learned before skill B). The relationship may be non-causal, in which case the ordering of skills does not matter and may indicate that both skills are prerequisites of another skill. In this study, we propose a simple, effective method of determining the strength of pre-to-post-requisite skill relationships. We then compare our results with a teacher-level survey about the strength of the relationships of the observed skills and find that the survey results largely confirm our findings in the data-driven approach.<br /><br /><strong>FRIDAY 4/29 - 1:00PM LAK SESSION "OVERCOMING OBSTACLES" (ST. TRINNEAN'S)</strong><br /><br />7C3: <span style="text-decoration: underline;">Enhancing the Efficiency and Reliability of Group Differentiation through Partial Credit</span><br />Yan Wang, Korinn Ostrow, Joseph Beck, Neil Heffernan<br /><br />Keywords: Partial Credit, Group Differentiation, Resampling, Randomized Controlled Trial, Data Mining.<br /><br />Abstract: The learning analytics community is situated at the forefront of optimizing data measurement and collection within educational realms. Many researchers in this field turn to intelligent tutoring systems or online learning platforms to conduct randomized controlled experiments with the goal of designing interventions that improve learning gains. The present work supports this avenue by promoting the use of rich system features, those that transcend binary correctness, to better define student performance. We argue that partial credit, defined as an algorithmic combination of binary correctness, hint usage, and attempt count, can benefit assessment and group differentiation. Using both experimental and non-experimental data sourced from ASSISTments, a popular online learning platform, a resampling technique is applied to compare partial credit with binary scoring in the context of significant group differentiation. Consistent benefits were observed for partial credit, with the most pronounced comparison resulting in a 39% reduction in the sample size required to reliably differentiate between user groups.<br /><br /><br /></p>