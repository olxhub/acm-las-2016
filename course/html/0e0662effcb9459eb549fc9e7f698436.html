<h3>Deployment of AXIS to collect explanations and learn policy from 150 participants</h3>
<p>The deployment case study was conducted with 150 people residing in the US who were recruited online to participate in an education research study, via Amazon Mechanical Turk.</p>
<p>All participants worked on the four math problems in a random order. For each problem, after entering an answer, they were told the correct answer. AXIS then displayed an explanation for why the answer was right, chosen by the explanation selection policy. At first the explanation pool was empty, so learners would instead see only the self-explanation prompt. This told learners to provide an explanation for how to solve the problem, because it would help them learn more deeply. We define the <em>AXIS Filtering Rule</em> to automatically discard explanations that were unlikely to be helpful to others. &nbsp;Specifically, AXIS added a learner's explanation to the explanation pool only if it was longer than 60 characters, the learner rated herself as having above average knowledge of how to solve problems like the current one, and the learner rated the likelihood of the explanation helping another learner as higher than 6, on a scale from 1 (Zero Chance) to 10 (Absolutely Likely). Once added to a problem's explanation pool, the explanation would be probabilistically selected for presentation to future learners working on the problem. A separate explanation pool and policy was maintained for each problem.</p>
<h3>Evaluation of AXIS Explanations and Policy with 564 participants</h3>
<p>To evaluate whether the AXIS system was able to collect and identify useful explanations from learners, it is important to check if AXIS successfully picks explanations that are helpful to future learners and discards ones that are not. Our experiment compared the quality of the explanations selected by AXIS to explanations AXIS filtered out and discarded, and to the original ASSISTments instructional designer's explanation for the same problem.&nbsp;The randomized experiment recruited 564 new people to participate in a HIT posted on Amazon Mechanical Turk.</p>
<p>The study consisted of a learning phase in which participants solved the four problems and provided ratings for explanations, followed by an assessment phase where they had to solve twelve problems without being given any feedback.&nbsp;</p>
<p><strong>Learning Phase</strong>. In this phase, participants were randomly assigned to a number of different conditions, in order to evaluate a wide range of explanations. One condition was solving <em>Problems with answers only</em>, standard practice for platforms that do not provide explanations. The other conditions all included explanations that were displayed after attempting a problem. For any one of the four problems, participants were randomly assigned to see one of the explanations from the AXIS pool for that problem, an explanation that was <em>Filtered out by AXIS</em>&nbsp;and not presented, or an original explanation <em>Written by Instructional Designer</em>&nbsp;at ASSISTments. This provided two important comparisons to AXIS's explanations (and policy), a lower bound in the form of explanations AXIS filtered out and did not present, and an upper bound in the form of the high quality explanations written by the original instructor.&nbsp;</p>
<p>Participants were prompted to rate how helpful these explanations were for learning (on a scale from 1 (Not Helpful At All) to 10 (Extremely Helpful)). In addition to rating explanations, this experiment asked learners to rate how likely they were to be successful at solving problems like the one they were about to attempt (or had just attempted). They made this rating on a scale from 1 (Extremely Unlikely) to 10 (Extremely Likely). By comparing these ratings before and after learners received different explanations, we had a more direct measure of the impact of different explanations on people's learning.&nbsp;</p>
<p><strong>Assessment Phase</strong>. The gold standard for the learnersourced explanations is that they positively impact objective behavioral measures of learning, especially transfer&nbsp;of knowledge to novel problems. The learning phase was followed by problems designed to assess whether participants learned from particular explanations. For each of the four original problems, there was an isomorphic problem where only the numbers and surface details (e.g., names in an expression) were changed. To measure transfer of the knowledge gained from explanations, participants attempted two problems that were novel but tested the same topic as the original problem (e.g., compound probability, using variables in algebraic expressions).</p>
<p><strong>Data from the evaluation experiment of the AXIS Explanations and Policy.</strong></p>
<p><img src="/static/table3-5.png" alt="" type="saveimage" target="[object Object]" preventdefault="function (){r.isDefaultPrevented=n}" stoppropagation="function (){r.isPropagationStopped=n}" stopimmediatepropagation="function (){r.isImmediatePropagationStopped=n}" isdefaultprevented="function t(){return!1}" ispropagationstopped="function t(){return!1}" isimmediatepropagationstopped="function t(){return!1}" width="538" height="221" /></p>