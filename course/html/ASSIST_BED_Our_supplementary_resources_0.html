<h3 style="text-align: justify; text-justify: inter-ideograph;">MONDAY 4/25 -&nbsp;5:30PM POSTER SESSION&nbsp;</h3>
<h4 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;"><a href="/static/FinalDemoL_S2016.pdf" target="[object Object]">Studying Learning at Scale with the ASSISTments TestBed</a>&nbsp;<o:p></o:p></span></h4>
<h5 style="text-align: justify; text-justify: inter-ideograph;">Korinn S. Ostrow, Neil T. Heffernan<o:p></o:p></h5>
<p class="MsoNormalCxSpFirst" style="text-align: justify; text-justify: inter-ideograph;"><em>Keywords:</em>&nbsp;ASSISTments TestBed; Randomized Controlled Experimentation at Scale; Authentic Learning Environments; Assessment of Learning Infrastructure</p>
<p class="MsoNormalCxSpMiddle" style="text-align: justify; text-justify: inter-ideograph;"><i>Abstract:&nbsp;</i>An interactive demonstration on how to design and implement randomized controlled experiments at scale within the ASSISTments TestBed, a new collaborative for educational research funded by the National Science Foundation (NSF). The Assessment of Learning infrastructure (ALI), a unique data retrieval and analysis tool, is also demonstrated.</p>
<h4 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;"><a href="https://drive.google.com/file/d/0B9sgUW9GkYhMem1hREI4YTM3TjA/view?usp=sharing" target="[object Object]">Optimizing the Amount of Practice in an On-Line Learning Platform</a>&nbsp;<o:p></o:p></span></h4>
<h6 style="text-align: justify; text-justify: inter-ideograph;">Kim M. Kelly, Neil T. Heffernan<o:p></o:p></h6>
<p class="MsoNormalCxSpFirst" style="text-align: justify; text-justify: inter-ideograph;"><i>Keywords:</i>&nbsp;On-Line learning platform; mastery learning; MOOCs; personalized instruction<o:p></o:p></p>
<p class="MsoNormalCxSpMiddle" style="text-align: justify; text-justify: inter-ideograph;"><i>Abstract:&nbsp;</i>Intelligent tutoring systems are known for providing customized learning opportunities for thousands of users. One feature of many systems is differentiating the amount of practice users receive. To do this, some systems rely on a threshold of consecutive correct responses. For instance, Khan Academy used to use ten correct in a row and now uses five correct in a row as the mastery threshold. The present research uses a series of randomized control trials, conducted in an online learning platform (eg., ASSISTments.org), to explore the effects of different thresholds of consecutive correct responses on learning. Results indicate that despite spending significantly more time practicing there is no significant difference on learning between two, three, four, or five consecutive correct responses. This suggests that systems, and MOOCS, can employ the simple rule of two or three consecutive correct responses when determining the amount of practice provided to users.</p>
<h4 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;">ASSISTments Dataset from Multiple Randomized Controlled Experiments<o:p></o:p></span></h4>
<h6 style="text-align: justify; text-justify: inter-ideograph;">Douglas Selent, Thanaporn Patikorn, Neil Heffernan<o:p></o:p></h6>
<p class="MsoNormalCxSpFirst" style="text-align: justify; text-justify: inter-ideograph;"><i>Keywords:</i>&nbsp;ASSISTments; Randomized Controlled Experiments; Dataset</p>
<p class="MsoNormalCxSpLast" style="text-align: justify; text-justify: inter-ideograph;"><i>Abstract:&nbsp;</i>In this paper, we present a dataset consisting of data generated from 22 previously and currently running randomized controlled experiments inside the ASSIStments online learning platform. This dataset provides data mining opportunities for researchers to analyze ASSISTments data in a convenient format across multiple experiments at the same time. The data preprocessing steps are explained in detail to inform researchers about how this dataset was generated. A list of column descriptions is provided to define the columns in the dataset and a set of summary statistics are presented to briefly describe the dataset.<o:p></o:p></p>
<h4 style="text-align: justify; text-justify: inter-ideograph;"><a href="https://drive.google.com/file/d/0B9sgUW9GkYhMQUc3X2ZtVFFaclE/view?usp=sharing" target="[object Object]">The&nbsp;Opportunity Count Model: A Flexible Approach to Modeling Student Performance</a></h4>
<h4 style="text-align: justify; text-justify: inter-ideograph;"></h4>
<p>Yan Wang, Korinn Ostrow, Seth Adjei, Neil Heffernan</p>
<p class="MsoNormalCxSpFirst" style="text-align: justify; text-justify: inter-ideograph;"><i>Keywords:&nbsp;</i>Opportunity Count; Random Forest; Student Modeling; Next Problem Correctness; Intelligent Tutoring System</p>
<p class="MsoNormalCxSpMiddle" style="text-align: justify; text-justify: inter-ideograph;"><i>Abstract:&nbsp;</i>Detailed performance data can be exploited to achieve stronger student models when predicting next problem correctness (NPC) within intelligent tutoring systems. However, the availability and importance of these details may differ significantly when considering opportunity count (OC), or the compounded sequence of problems a student experiences within a skill. Inspired by this intuition, the present study introduces the Opportunity Count Model (OCM), a unique approach to student modeling in which separate models are built for differing OCs rather than creating a blanket model that encompasses all OCs. We use Random Forest (RF), which can be used to indicate feature importance, to construct the OCM by considering detailed performance data within tutor log files. Results suggest that OC is significant when modeling student performance and that detailed performance data varies across OCs.</p>
<h3 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;">TUESDAY 4/26 - 1:30PM SESSION #5 (PENTLAND E+W)<o:p></o:p></span></h3>
<h4 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;">5A:&nbsp;<a href="https://web.eecs.umich.edu/~wlasecki/pubs/axis_las2016.pdf" target="[object Object]">AXIS: Generating Explanations at Scale with Learnersourcing and Machine Learning&nbsp;</a><o:p></o:p></span></h4>
<h6 style="text-align: justify; text-justify: inter-ideograph;">Joseph Jay Williams, Juho Kim, Anna Rafferty, Samuel Maldonado, Krzysztof Z. Gajos, Walter S. Lasecki, Neil Heffernan<o:p></o:p></h6>
<p class="MsoNormalCxSpFirst" style="text-align: justify; text-justify: inter-ideograph;"><i>Keywords:</i>&nbsp;Explanation; learning at scale; crowdsourcing; learnersourcing; machine learning; adaptive learning</p>
<p class="MsoNormalCxSpLast" style="text-align: justify; text-justify: inter-ideograph;"><i>Abstract:&nbsp;</i>While explanations may help people learn by providing information about why an answer is correct, many problems on online platforms lack high-quality explanations. This paper presents AXIS (Adaptive eXplanation Improvement System), a system for obtaining explanations. AXIS asks learners to generate, revise, and evaluate explanations as they solve a problem, and then uses machine learning to dynamically determine which explanation to present to a future learner, based on previous learners' collective input. Results from a case study deployment and a randomized experiment demonstrate that AXIS elicits and identifies explanations that learners find helpful. Providing explanations from AXIS also objectively enhanced learning, when compared to the default practice where learners solved problems and received answers without explanations. The rated quality and learning benefit of AXIS explanations did not differ from explanations generated by an experienced instructor.<o:p></o:p></p>
<h3 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;">THURSDAY 4/28 - 10:30AM LAK SESSION "THEORETICAL AND CONCEPTUAL MODELS" (PRESONTFIELD)<o:p></o:p></span></h3>
<h4 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;">4D2:&nbsp;<a href="https://drive.google.com/file/d/0B9sgUW9GkYhMMFhNbkpjTE5kNkE/view?usp=sharing" target="[object Object]">The Assessment of Learning Infrastructure (ALI): The Theory, Practice, and Scalability of Automated Assessment</a><o:p></o:p></span></h4>
<h6 style="text-align: justify; text-justify: inter-ideograph;">Korinn Ostrow, Doug Selent, Yan Wang, Eric Van Inwegen, Neil Heffernan, Joseph Jay Williams<o:p></o:p></h6>
<p class="MsoNormalCxSpFirst" style="text-align: justify; text-justify: inter-ideograph;"><i>Keywords</i>:<b>&nbsp;</b>Assessment of Learning Infrastructure, Automated Analysis, Randomized Controlled Experiments at Scale, The ASSISTments TestBed, Universal Data Reporting, Tools for Learning Analytics.</p>
<p class="MsoNormalCxSpMiddle" style="text-align: justify; text-justify: inter-ideograph;"><i>Abstract<b>:&nbsp;</b></i>Researchers invested in K-12 education struggle not just to enhance pedagogy, curriculum, and student engagement, but also to harness the power of technology in ways that will optimize learning. Online learning platforms offer a powerful environment for educational research at scale. The present work details the creation of an automated system designed to provide researchers with insights regarding data logged from randomized controlled experiments conducted within the ASSISTments TestBed. The Assessment of Learning Infrastructure (ALI) builds upon existing technologies to foster a symbiotic relationship beneficial to students, researchers, the platform and its content, and the learning analytics community. ALI is a sophisticated automated reporting system that provides an overview of sample distributions and basic analyses for researchers to consider when assessing their data. ALI&rsquo;s benefits can also be felt at scale through analyses that crosscut multiple studies to drive iterative platform improvements while promoting personalized learning.</p>
<h3 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;">FRIDAY 4/29 - 1:00PM LAK SESSION "PREDICTIVE MODELLING" (PENTLAND WEST)<o:p></o:p></span></h3>
<h4 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;">7D2: <a href="http://users.wpi.edu/~saadjei/papers/LAK16_paper_26.pdf" target="[object Object]">Predicting Student Performance on Post-requisite Skills Using Prerequisite Skill Data: An alternative method for refining Prerequisite Skill Structures</a><o:p></o:p></span></h4>
<h6 style="text-align: justify; text-justify: inter-ideograph;">Seth Adjei, Anthony Botelho, Neil Heffernan<o:p></o:p></h6>
<p class="MsoNormalCxSpFirst" style="text-align: justify; text-justify: inter-ideograph;"><i>Keywords</i>:<b>&nbsp;</b>Prerequisite Structures, learning maps, skill relationships, refinements, PLACEments<o:p></o:p></p>
<p class="MsoNormalCxSpMiddle" style="text-align: justify; text-justify: inter-ideograph;"><i>Abstract:&nbsp;</i>Prerequisite skill structures have been closely studied in past years&nbsp;leading to many data-intensive methods aimed at refining such&nbsp;structures. While many of these proposed methods have yielded&nbsp;success, defining and refining hierarchies of skill relationships are&nbsp;often difficult tasks. The relationship between skills in a graph&nbsp;could either be causal, therefore, a prerequisite relationship (skill&nbsp;A must be learned before skill B). The relationship may be non-causal, in which case the ordering of skills does not matter and&nbsp;may indicate that both skills are prerequisites of another skill. In&nbsp;this study, we propose a simple, effective method of determining&nbsp;the strength of pre-to-post-requisite skill relationships. We then&nbsp;compare our results with a teacher-level survey about the strength&nbsp;of the relationships of the observed skills and find that the survey&nbsp;results largely confirm our findings in the data-driven approach.</p>
<h3 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;">FRIDAY 4/29 - 1:00PM LAK SESSION "OVERCOMING OBSTACLES" (ST. TRINNEAN'S)<o:p></o:p></span></h3>
<h4 style="text-align: justify; text-justify: inter-ideograph;"><span style="color: windowtext;">7C3:<a href="https://drive.google.com/file/d/0B9sgUW9GkYhMSG84RkFhaWpsYzQ/view?usp=sharing" target="[object Object]"> Enhancing the Efficiency and Reliability of Group Differentiation through Partial Credit</a><o:p></o:p></span></h4>
<h6 style="text-align: justify; text-justify: inter-ideograph;">Yan Wang, Korinn Ostrow, Joseph Beck, Neil Heffernan<o:p></o:p></h6>
<p class="MsoNormalCxSpFirst" style="text-align: justify; text-justify: inter-ideograph;"><i>Keywords:&nbsp;</i>Partial Credit, Group Differentiation, Resampling, Randomized Controlled Trial, Data Mining.<o:p></o:p></p>
<p class="MsoNormalCxSpMiddle" style="text-align: justify; text-justify: inter-ideograph;"><i>Abstract:&nbsp;</i>The learning analytics community is situated at the forefront of&nbsp;optimizing data measurement and collection within educational&nbsp;realms. Many researchers in this field turn to intelligent tutoring&nbsp;systems or online learning platforms to conduct randomized&nbsp;controlled experiments with the goal of designing interventions&nbsp;that improve learning gains. The present work supports this&nbsp;avenue by promoting the use of rich system features, those that&nbsp;transcend binary correctness, to better define student performance.&nbsp;We argue that partial credit, defined as an algorithmic&nbsp;combination of binary correctness, hint usage, and attempt count,&nbsp;can benefit assessment and group differentiation. Using both&nbsp;experimental and non-experimental data sourced from&nbsp;ASSISTments, a popular online learning platform, a resampling&nbsp;technique is applied to compare partial credit with binary scoring&nbsp;in the context of significant group differentiation. Consistent&nbsp;benefits were observed for partial credit, with the most&nbsp;pronounced comparison resulting in a 39% reduction in the&nbsp;sample size required to reliably differentiate between user groups.&nbsp;</p>